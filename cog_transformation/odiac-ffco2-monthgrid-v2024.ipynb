{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d013be06",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: ODIAC Fossil Fuel CO₂ Emissions\n",
    "description: Documentation of data transformation\n",
    "author: Paridhi Parajuli\n",
    "date: July 1, 2025\n",
    "execute:\n",
    "  freeze: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fba699c",
   "metadata": {},
   "source": [
    "This script was used to transform the ODIAC Fossil Fuel CO₂ Emissions dataset from GeoTIFF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccac07ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "import re\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import boto3\n",
    "import os\n",
    "import gzip,shutil, wget\n",
    "import s3fs\n",
    "import hashlib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68acb029-caef-4d08-bf59-fea0ccea8370",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "session = boto3.session.Session()\n",
    "s3_client = session.client(\"s3\")\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "data_dir = \"data/\"\n",
    "dataset_name = \"odiac-ffco2-monthgrid-v2024\"\n",
    "cog_data_bucket = \"ghgc-data-store-develop\"\n",
    "cog_data_prefix= f\"transformed_cogs/{dataset_name}\"\n",
    "cog_checksum_prefix= \"checksum\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb2034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the checksum of raw files\n",
    "checksum_dict ={}\n",
    "for year in range(2000,2023):\n",
    "    checksum_url = f\"https://db.cger.nies.go.jp/nies_data/10.17595/20170411.001/odiac2024/1km_tiff/{year}/odiac2024_1km_checksum_{year}.md5.txt\"\n",
    "    response = requests.get(checksum_url)\n",
    "    content = response.text\n",
    "    tmp={}\n",
    "    \n",
    "    # Split the content into lines\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        checksum, filename = line.split()\n",
    "        tmp[filename[:-3]] = checksum\n",
    "    checksum_dict.update(tmp)\n",
    "checksum_dict = {k: v for k, v in checksum_dict.items() if k.endswith('.tif')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_md5(file_path):\n",
    "    \"\"\"\n",
    "    Calculate the MD5 hash of a file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the file.\n",
    "\n",
    "    Returns:\n",
    "    str: The MD5 hash of the file.\n",
    "    \"\"\"\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa50806-47ac-47f3-8019-40114dec0419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to download raw ODIAC data in your local machine\n",
    "\n",
    "# Creating  a base directory for ODIAC data\n",
    "if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "checksum_dict_local={}\n",
    "# Download and unzip data for the years you want\n",
    "for year in range(2000,2023):\n",
    "    year_dir = os.path.join(data_dir, str(year))\n",
    "    checksum_download_link = f\"https://db.cger.nies.go.jp/nies_data/10.17595/20170411.001/odiac2024/1km_tiff/{year}/odiac2024_1km_checksum_{year}.md5.txt\"\n",
    "    wget.download(checksum_download_link, year_dir)\n",
    "    # Make a subfolder for each year\n",
    "    if not os.path.exists(year_dir):\n",
    "        os.makedirs(year_dir)\n",
    "\n",
    "    for month in range(1,13):\n",
    "        month = f\"{month:02d}\"\n",
    "        download_link = f\"https://db.cger.nies.go.jp/nies_data/10.17595/20170411.001/odiac2024/1km_tiff/{year}/odiac2024_1km_excl_intl_{str(year)[-2:]}{month}.tif.gz\"\n",
    "        target_folder = f\"{data_dir}/{year}/\"\n",
    "        fname = os.path.basename(download_link)\n",
    "        target_path = os.path.join(target_folder, fname)\n",
    "\n",
    "        # Download the file\n",
    "        wget.download(download_link, target_path)\n",
    "\n",
    "        # Unzip the file\n",
    "        with gzip.open(target_path, 'rb') as f_in:\n",
    "            with open(target_path[:-3], 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "                \n",
    "        # Calculate checksum of the .gz file \n",
    "        checksum_dict_local[target_path.split(\"/\")[-1][:-3]]=calculate_md5(target_path)\n",
    "        \n",
    "        # Remove the zip file\n",
    "        os.remove(target_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bc81a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the checksums match\n",
    "checksum_dict_local == checksum_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fcdedc-4ce2-4875-9f7b-a98e004ae559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of years you want to run the transformation on\n",
    "fold_names=[str(i) for i in range(2000,2024)]\n",
    "\n",
    "for fol_ in fold_names:\n",
    "    names= os.listdir(f\"{data_dir}{fol_}\")\n",
    "    names= [name for name in names if name.endswith('.tif')]\n",
    "    print(\"For year: \" ,fol_)\n",
    "    for name in names:\n",
    "        xds = xarray.open_dataarray(f\"{data_dir}{fol_}/{name}\")\n",
    "        filename = name.split(\"/ \")[-1]\n",
    "        filename_elements = re.split(\"[_ .]\", filename)\n",
    "        \n",
    "        # Remove the extension\n",
    "        filename_elements.pop()\n",
    "        # Extract and insert date of generated COG into filename\n",
    "        filename_elements[-1] = fol_ + filename_elements[-1][-2:]\n",
    "\n",
    "        # Replace 0 values  with -9999\n",
    "        xds = xds.where(xds!=0, -9999)\n",
    "        xds.rio.set_spatial_dims(\"x\", \"y\", inplace=True)\n",
    "        xds.rio.write_nodata(-9999, inplace=True)\n",
    "        xds.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "\n",
    "        cog_filename = \"_\".join(filename_elements)\n",
    "        cog_filename = f\"{cog_filename}.tif\"\n",
    "\n",
    "        # Write the cog file to s3 \n",
    "        with tempfile.NamedTemporaryFile() as temp_file:\n",
    "            xds.rio.to_raster(\n",
    "                temp_file.name,\n",
    "                driver=\"COG\",\n",
    "                compress=\"DEFLATE\"\n",
    "            )\n",
    "            s3_client.upload_file(\n",
    "                Filename=temp_file.name,\n",
    "                Bucket=cog_data_bucket,\n",
    "                Key=f\"{cog_data_prefix}/{cog_filename}\",\n",
    "            )\n",
    "\n",
    "        print(f\"Generated and saved COG: {cog_filename}\")\n",
    "\n",
    "print(\"ODIAC COGs generation completed!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is used to calculate the SHA for each COG file and store in a JSON.\n",
    "\n",
    "def get_all_s3_keys(bucket, model_name, ext):\n",
    "    \"\"\"Get a list of all keys in an S3 bucket.\"\"\"\n",
    "    keys = []\n",
    "\n",
    "    kwargs = {\"Bucket\": bucket, \"Prefix\": f\"{model_name}/\"}\n",
    "    while True:\n",
    "        resp = s3_client.list_objects_v2(**kwargs)\n",
    "        for obj in resp[\"Contents\"]:\n",
    "            if obj[\"Key\"].endswith(ext) and \"historical\" not in obj[\"Key\"]:\n",
    "                keys.append(obj[\"Key\"])\n",
    "\n",
    "        try:\n",
    "            kwargs[\"ContinuationToken\"] = resp[\"NextContinuationToken\"]\n",
    "        except KeyError:\n",
    "            break\n",
    "\n",
    "    return keys\n",
    "\n",
    "keys = get_all_s3_keys(cog_data_bucket, cog_data_prefix,\".tif\")\n",
    "\n",
    "\n",
    "def compute_sha256(url):\n",
    "    \"\"\"Compute SHA-256 checksum for a given file.\"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with fs.open(url) as f:\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "sha_mapping = {}\n",
    "for key in keys:\n",
    "    sha_mapping[key.split(\"/\")[-1]]=compute_sha256(f\"s3://{cog_data_bucket}/{key}\")\n",
    "\n",
    "\n",
    "json_data = json.dumps(sha_mapping, indent=4)\n",
    "s3_client.put_object(Bucket=cog_data_bucket, Key=f\"{cog_checksum_prefix}/{dataset_name}.json\", Body=json_data)\n",
    "\n",
    "print(\"Checksums created for ODIAC!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
